[
  
  {
    "title": "Following Along with UIUC&#39;s CS 598 PROOF AUTOMATION",
    "url": "/posts/proof_automation/",
    "categories": "Proof Automation",
    "tags": "verification, proof assistants",
    "date": "2022-01-03 00:00:00 -0600",
    





    "snippet": "Dr. Talia Ringer is teaching a course on proof automation at the University of Illinois at Urbana-Champaign. I’ve been a programming language nerd since dabbling with Haskell and the like in undergrad, and I’m very interested in the whole area of “proving things about computation”. Naturally, this falls squarely into that intersection, so I’m going to be following along with the course as best as I can manage this semester. This ongoing blog post will be where I post my notes, thoughts, and some solutions (only the non-directly-copyable sort).If you’re interested in a discussion on anything in this post, please reply to my relevant tweet.Homework 1Chapters 1 and 3 of QED at Large: A Survey of Engineering of Formally Verified Software by Talia Ringer, Karl Palmskog, Ilya Sergey, Milos Gligoric, and Zachary Tatlock.Reading Thoughts:The reading assignment served as a good introduction to the history of interactive verification. I think would have liked some companion pieces on the real and potential broader impacts of verification in the world. That would have felt more complete in an “end-to-end” sense and helped me get excited about the topic.I have one question about the paper: what is the difference between program verifiers and the application of proof assistants to verifying programs? Section 1.2 of the paper discusses its scope, and notes that it will focus on the latter, but not the former. I’m unsure of the relationship between the topics specifically omitted from the scope, i.e. program verifiers, theorem provers, and constraint solvers, and the topic of focus, proof assistants in the context of software engineering.Homework Prompt:Section 3.3 lists exactly one machine learning system that has been verified in a proof assistant. Based on what you know so far, do you think there is much hope for verifying state-of-the-art machine learning systems inside of a proof assistant? If so, why and how? And if not, what do you think it would take to be able to formally verify state-of-the-art machine learning systems?Response:In short, if “state-of-the-art” refers to the darlings of the machine learning research community, I don’t think they will be touched by formal verification in any meaningful way for a long time. On the other hand, if state-of-the-art simply means the models being employed today on real problems, then I think there’s a lot more hope. The central problem with the models being studied by the ML research community (e.g., transformers and large language models), is the enchantment the ML community has with increasingly large parameter counts and increasingly sophisticated architectures. This trend makes reasoning about these models more and more difficult as it progresses, outpacing the progress of the verification community. What is needed in this context is time for computing resources and software to catch up. On the other hand, where the size of a model can be kept reasonable, and its use is sufficiently sensitive so as to warrant more effort on verification (e.g., medicine, cyber-physical systems), I think that it is possible to define specific properties that models must satisfy and to define the models themselves as well as their settings in a concrete enough way so as to be amenable to being formally reasoned about. I think a next step in this direction is in the intersection of formal reasoning and probability, as proving properties of statistical machine learning models can be prohibitively computationally expensive.Homework 2the ACM Ethics CodeReading thoughts:The ACM Ethics Code doesn’t seem to be a very serious set of ethical guidelines to me. Much of it is written and defined in a hand-wringing way that doesn’t really take many strong positions. Excerpts like “unless there is a compelling ethical reason to do otherwise”, “as much as possible”, and “ensure that [] harm is ethically justified”, really seem to undermine the strength of some of the code’s reccommendations.I will say though, I’m really glad they didn’t leave out an important one: “Appropriate human-computer ergonomic standards should be used in the workplace”.Homework prompt:Imagine that you are a professor in charge of a research group, working on proof automation that makes it easier to prove operating systems secure. DARPA offers you $10,000,000 in support of your research, with the caveat that you must demonstrate the success of your automation to build a verified operating system that makes a drone resilient to remote hijacking. Do you take the money? What ethical concerns factor into your decision? What points from the ACM Ethics Code do you find relevant?Response:I like this prompt because it is a sufficiently thick account to make an ethical argument I’ve been fencing with my advisor over for years. I believe that a person who contributes to the development of a technology that can forseeably be used for a specific set of ends, is responsible for those ends to a degree proportional to how much their efforts hastened those ends. Put simply, if your work makes something happen that wouldn’t have otherwise happened, you are partially to blame. This is the tip of my spear when arguing that technology isn’t neutral, and something I should expand on in another blog post sometime.The immediate relevancy to the question at hand is that my contribution to the safety of military drones, which are almost certainly going to be used to kill more than any other purpose (despite all those ads trying to convince of the contrary), plays a role in my partial culpability for those killings. Obviously there are many details that can be explored, including the positive outcomes of the work, but I’m not personally interested in moral calculus. I’d rather not contribute to killings where I can avoid it, as a rule. I could not take the money, even if it meant a rockier path down the same research road.I don’t think any of the ACM code applies to this question because it seems to have been written in a way that makes collaborating with power excusable in many ways. The point that seems to most directly relate to the question is 1.2, “Avoid harm”. However, this excerpt totally removes the teeth in this situation: “When harm is an intentional part of the system, those responsible are obligated to ensure that the harm is ethically justified.”Homework 3Homework 4First class excercise with Agda. To kick things off, I’d like to re-iterate my hatred of Emacs and my wishes that there was another good medium for working with the likes of Lisp and now Agda. Setting everything up was pretty easy until I had to figure out how to do anything at all with Emacs. I’m a Vim gal, but I’ve always been pretty flexible with my editors. All I’ll say is that everyone I know who uses Emacs is in wrist braces for carpal tunnel-type symptoms.DISCUSSION QUESTION: What was it like constructing these proofs directly,as terms in a programming language? What did you find challenging aboutthis experience, if anything? What did you find helpful, if anything? Didyou get stuck at any point, and if so, where and why? Where do you wishyou’d had more automation to help you out?My pain points with the process were mainly being unsure of what the error messages actually mean about my program. I understand that the types aren’t unifying like I want them to, but something that bridges the gap between my potential intentions and what the compiler expects would be really helpful. Rust does a fantastic job of this with its error messages, and this trial-and-error effort is redoubling my appreciation for it.I got nerd sniped really hard by this assignment. I got stuck at the last hole because I didn’t really understand how to use substitution as defined (mostly because the notion of a “property” isn’t clear). I think the lack of references to Agda syntax and basics is a definite gap in this assignment. I know Haskell pretty well, but that only goes so far here.The fact that we can prove so much so easily is really neat here. I’m used to proofs being numerical and reqruiring a lot of effort, so being able to write proofs with small, algebraic terms is fun."
  },
  
  {
    "title": "Star Sets",
    "url": "/posts/starsets/",
    "categories": "Tutorials",
    "tags": "verification, star sets",
    "date": "2021-09-21 00:00:00 -0500",
    





    "snippet": "The star set, introduced in (Tran et al., 2019), is a construction that is central to my work right now, so I wrote up this tutorial to help explain it in a simple, motivated way to my collaborators. I’m sharing it here now for my students, and so that I have something to which I can direct the curious.(Note: this post is a work in progress, expect it to change)MotivationStar sets are a verification method created for calculating reachability1, i.e., answering the question of “if the set of all possible inputs is $X$, what are all possible outputs $Y$?”.The typical use of this question and answer in control is to decide whether the set of possible outputs overlaps at all with an unsafe set.Any non-empty intersection indicates an intervention to prevent the potentially unsafe action.DefinitionsThis section is going to use a lot of notation for readers who benefit from such things. If you’re interested in the main idea, don’t worry about the typeset math, just read the text and you’ll be able to understand everything that follows just as well.We’ll start by understanding what Star Sets are trying to accomplish, to represent neural network transformations on entire sets rather than individual inputs.The simplest case works best with DNNs defined only with fully connected (FC) layers and ReLU activations, so that’s what we’ll use in this tutorial.To establish some definitions, neural networks can be expressed as $y = f(x)$ where\\[x\\in\\mathbb{R}^m, y\\in\\mathbb{R}^n, f: \\mathbb{R}^m\\to\\mathbb{R}^n\\]We’re interested in efficiently calculating $Y = f(X)$ where $f$ is the same, but instead of indiviual inputs, we want to work with sets,\\[X\\subseteq\\mathbb{R}^m, Y=\\{y|y=f(x), x\\in X\\}\\]The question is how to represent the sets so that $Y$ can be calculated efficiently? Because we’re only considering neural networks with FC layers and ReLU activations, we’ll rewrite our $n$-layer DNN definition as\\[\\begin{aligned}f &amp;amp;= f_n \\circ f_{n-1} \\circ\\cdots\\circ f_2\\circ f_1 \\\\&amp;amp;= \\text{aff}_n\\circ\\text{ReLU}_{n-1}\\circ\\text{aff}_{n-1}\\circ\\cdots\\circ\\text{ReLU}_1\\circ\\text{aff}_1\\end{aligned}\\]where $f_i$ is the $i$th layer of the network and the final layer of the network isn’t activated.Using a set as input to each layer, choosing ReLU activations means that each layer’s output set to being closed under affine and ReLU operations.Post ScriptBibliographyTran, H.-D., Lopez, D. M., Musau, P., Yang, X., Nguyen, L. V., Xiang, W., &amp;amp; Johnson, T. T. (2019). Star-based reachability analysis of deep neural networks. International Symposium on Formal Methods, 670–686.Footnotes            I’m using them for something different, but that’s for another post. &amp;#8617;      "
  }
  
]

